{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c908866",
   "metadata": {},
   "source": [
    "# Falcon 9 & Falcon Heavy Launches - Web Scraping\n",
    "**Author:** Muhammad Munawar Shahzad \n",
    "\n",
    "**Date:** August 2025  \n",
    "**Project:** IBM Applied Data Science Capstone â€“ SpaceX Falcon 9\n",
    "**Author:** Muhammad Munawar Shahzad \n",
    "\n",
    "\n",
    "**Repository:** `falcon9_project`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5b409",
   "metadata": {},
   "source": [
    "- Learn how to perform web scraping using Python's `requests` and `BeautifulSoup`.\n",
    "- Extract launch data from Wikipedia.\n",
    "- Store the extracted data in a structured format (CSV) for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3acd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries for web scraping and data handling\n",
    "import requests                      # To fetch HTML content from the web\n",
    "from bs4 import BeautifulSoup        # To parse and navigate HTML\n",
    "import pandas as pd                  # To store and clean data in a tabular format\n",
    "\n",
    "# Display a message to confirm imports\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda3fcf",
   "metadata": {},
   "source": [
    "We chose the Wikipedia page for \"List of Falcon 9 and Falcon Heavy launches\" because:\n",
    "- It contains comprehensive, up-to-date launch records.\n",
    "- Wikipedia pages are publicly accessible and have structured tables.\n",
    "- The data can be scraped without requiring authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056e47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the target URL\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Falcon_9_and_Falcon_Heavy_launches\"\n",
    "\n",
    "# Step 3: Fetch the HTML content using requests\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 4: Check status code to ensure the request was successful\n",
    "print(\"Status Code:\", response.status_code)\n",
    "\n",
    "# Step 5: Store HTML content in a variable\n",
    "html_content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfcefafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables found: 5\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create a BeautifulSoup object to parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Step 7: Find all tables on the page\n",
    "html_tables = soup.find_all('table', {\"class\": \"wikitable\"})\n",
    "\n",
    "# Step 8: Check number of tables found\n",
    "print(\"Number of tables found:\", len(html_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dac1384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: ['Flight No.', 'Date andtime (UTC)', 'Version,booster[i]', 'Launchsite', 'Payload[j]', 'Payload mass', 'Orbit', 'Customer', 'Launchoutcome', 'Boosterlanding', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', 'FH 10', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', 'FH 11', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417']\n",
      "Number of rows extracted: 272\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Extract column names from the first table\n",
    "first_launch_table = html_tables[0]\n",
    "\n",
    "column_names = []\n",
    "for th in first_launch_table.find_all('th'):\n",
    "    col_name = th.text.strip()\n",
    "    if col_name:\n",
    "        column_names.append(col_name)\n",
    "\n",
    "print(\"Column Names:\", column_names)\n",
    "\n",
    "# Step 10: Extract row data\n",
    "table_rows = []\n",
    "for tr in first_launch_table.find_all('tr')[1:]:  # Skip header row\n",
    "    cells = tr.find_all(['td', 'th'])\n",
    "    row = [cell.text.strip() for cell in cells]\n",
    "    if row:\n",
    "        table_rows.append(row)\n",
    "\n",
    "print(\"Number of rows extracted:\", len(table_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca6d507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flight No.         Date andtime (UTC) Version,booster[i]  \\\n",
      "0        286   January 3, 202403:44[24]        F9B5B1082â€‘1   \n",
      "1        287   January 3, 202423:04[25]       F9B5B1076â€‘10   \n",
      "2        288   January 7, 202422:35[29]       F9B5B1067â€‘16   \n",
      "3        289  January 14, 202408:59[31]       F9B5B1061â€‘18   \n",
      "4        290  January 15, 202401:52[32]       F9B5B1073â€‘12   \n",
      "\n",
      "              Launchsite                         Payload[j]  \\\n",
      "0      Vandenberg,SLCâ€‘4E  Starlink:GroupÂ 7-9(22Â satellites)   \n",
      "1  Cape Canaveral,SLCâ€‘40                            Ovzon-3   \n",
      "2  Cape Canaveral,SLCâ€‘40  Starlink:GroupÂ 6-35(23satellites)   \n",
      "3      Vandenberg,SLCâ€‘4E  Starlink:GroupÂ 7-10(22satellites)   \n",
      "4  Cape Canaveral,SLCâ€‘40  Starlink:GroupÂ 6-37(23satellites)   \n",
      "\n",
      "             Payload mass Orbit Customer Launchoutcome    Boosterlanding  \n",
      "0  ~16,800Â kg (37,000Â lb)   LEO   SpaceX       Success  Success (OCISLY)  \n",
      "1     1,800Â kg (4,000Â lb)   GTO    Ovzon       Success    Success (LZâ€‘1)  \n",
      "2  ~17,100Â kg (37,700Â lb)   LEO   SpaceX       Success    Success (ASOG)  \n",
      "3  ~16,700Â kg (36,800Â lb)   LEO   SpaceX       Success  Success (OCISLY)  \n",
      "4  ~17,100Â kg (37,700Â lb)   LEO   SpaceX       Success    Success (ASOG)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Falcon_9_and_Falcon_Heavy_launches\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 1: Find all tables with class 'wikitable'\n",
    "tables = soup.find_all('table', {\"class\": \"wikitable\"})\n",
    "\n",
    "# Step 2: Select first table (you can loop later if multiple needed)\n",
    "first_table = tables[0]\n",
    "\n",
    "# Step 3: Extract header only from the first row\n",
    "header_row = first_table.find('tr')\n",
    "column_names = [th.get_text(strip=True) for th in header_row.find_all('th')]\n",
    "\n",
    "# Step 4: Extract all rows after the header\n",
    "table_rows = []\n",
    "for tr in first_table.find_all('tr')[1:]:\n",
    "    cells = tr.find_all(['td', 'th'])\n",
    "    row = [cell.get_text(strip=True) for cell in cells]\n",
    "    if row and len(row) == len(column_names):  # only keep rows with correct column length\n",
    "        table_rows.append(row)\n",
    "\n",
    "# Step 5: Create DataFrame safely\n",
    "df = pd.DataFrame(table_rows, columns=column_names)\n",
    "\n",
    "# Step 6: Clean and show\n",
    "df.replace('\\n', ' ', regex=True, inplace=True)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4efd5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Raw web scraping data saved to: d:\\Projects\\falcon9_project\\data\\raw\\falcon9_web_scraped.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# Step 14: Save cleaned data to processed folder\n",
    "# -------------------------------------------------\n",
    "# Save scraped data to raw folder\n",
    "# -------------------------------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "# Define project folders (adjust if not already defined)\n",
    "PROJECT_ROOT = Path.cwd().parents[0]  # agar notebook 'notebooks/' me hai\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define file path\n",
    "RAW_DATA_PATH = RAW_DIR / \"falcon9_web_scraped.csv\"\n",
    "\n",
    "# Save DataFrame to raw folder\n",
    "df.to_csv(RAW_DATA_PATH, index=False)\n",
    "print(f\"ðŸ’¾ Raw web scraping data saved to: {RAW_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4c1ff",
   "metadata": {},
   "source": [
    "- Fetched HTML content from Wikipedia.\n",
    "- Parsed and navigated HTML using BeautifulSoup.\n",
    "- Extracted launch table data.\n",
    "- Cleaned and structured the data using pandas.\n",
    "- Saved the data to CSV for later use in analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16e2a2",
   "metadata": {},
   "source": [
    "In the next notebook, we will:\n",
    "- Load the cleaned CSV file.\n",
    "- Perform exploratory data analysis (EDA).\n",
    "- Create visualizations for Falcon 9 & Falcon Heavy launches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303343d",
   "metadata": {},
   "source": [
    "- Wikipedia: [List of Falcon 9 and Falcon Heavy launches](https://en.wikipedia.org/wiki/List_of_Falcon_9_and_Falcon_Heavy_launches)\n",
    "- BeautifulSoup Documentation: https://www.crummy.com/software/BeautifulSoup/\n",
    "- Pandas Documentation: https://pandas.pydata.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
